# XGB算法梳理

XGBoost是boosting算法的其中一种。Boosting算法的思想是将许多弱分类器集成在一起形成一个强分类器。因为XGBoost是一种提升树模型，所以它是将许多树模型集成在一起，形成一个很强的分类器。而所用到的树模型则是CART回归树模型。

---
## XGBoost算法思想
该算法思想就是不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数，去拟合上次预测的残差。当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数，最后只需要将每棵树对应的分数加起来就是该样本的预测值。

## XGBoost原理 
XGBoost目标函数定义为：
$$Obj=\sum_{i=1}^nl(y_i,\hat{y_i}) + \sum_{k=1}^KΩ(f_k) $$

目标函数由两部分构成，第一部分用来衡量预测分数和真实分数的差距，另一部分则是正则化项。正则化项同样包含两部分，T表示叶子结点的个数，w表示叶子节点的分数。γ可以控制叶子结点的个数，λ可以控制叶子节点的分数不会过大，防止过拟合。新生成的树是要拟合上次预测的残差的，即当生成t棵树后，预测分数可以写成：
$$\hat{y}_i^{(t)} =\hat{y}_i^{(t-1)} +f_t(x_i) $$
可以将目标函数改写成：
$$θ^{(t)} = \sum_{i=1}^nl(y_i,\hat{y}_i^{t-1} + f_t(x_i)) +Ω(f_t)$$ 

## 分裂结点算法
正如上文说到，基于空间切分去构造一颗决策树是一个NP难问题，我们不可能去遍历所有树结构，因此，XGBoost使用了和CART回归树一样的想法，利用贪婪算法，遍历所有特征的所有特征划分点，不同的是使用上式目标函数值作为评价函数。具体做法就是分裂后的目标函数值比单子叶子节点的目标函数的增益，同时为了限制树生长过深，还加了个阈值，只有当增益大于该阈值才进行分裂。同时可以设置树的最大深度、当样本权重和小于设定阈值时停止生长去防止过拟合。

## 针对稀疏数据的算法（缺失值处理）
当样本的第i个特征值缺失时，无法利用该特征进行划分时，XGBoost的想法是将该样本分别划分到左结点和右结点，然后计算其增益，哪个大就划分到哪边。

## XGBoost的优点
1.使用许多策略去防止过拟合，如：正则化项、Shrinkage and Column Subsampling等。

2. 目标函数优化利用了损失函数关于待求函数的二阶导数

3.支持并行化，这是XGBoost的闪光点，虽然树与树之间是串行关系，但是同层级节点可并行。具体的对于某个节点，节点内选择最佳分裂点，候选分裂点计算增益用多线程并行。训练速度快。

4.添加了对稀疏数据的处理。

5.交叉验证，early stop，当预测结果已经很好的时候可以提前停止建树，加快训练速度。

6.支持设置样本权重，该权重体现在一阶导数g和二阶导数h，通过调整权重可以去更加关注一些样本。